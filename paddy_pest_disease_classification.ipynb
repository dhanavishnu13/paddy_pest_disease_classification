{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhanavishnu13/paddy_pest_disease_classification/blob/main/paddy_pest_disease_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Deployed in Huggingface Spaces](https://huggingface.co/spaces/vishnu23/Paddy_Leaf_Disease_Classification_using_CNN)"
      ],
      "metadata": {
        "id": "ItQQANSvVLjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#connect google drive to load training images\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0jKDsl13rl1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1:  Import the required libraries and packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import keras\n",
        "import tensorflow\n",
        "from keras.preprocessing.image  import ImageDataGenerator\n",
        "from keras.layers import Flatten, Dense\n",
        "from keras import layers,models\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
      ],
      "metadata": {
        "id": "WgJhY6eMW10E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2: Loading the dataset\n",
        "'''\n",
        "I have the paddy pest disease classification dataset in my drive.\n",
        "ImageDataGenerator can apply various transformations, such as rotation, flipping, scaling, shearing,\n",
        "and more, to generate new variations of the input images in real-time.\n",
        "This helps the model to generalize better.\n",
        "'''\n",
        "train_datagen = ImageDataGenerator(zoom_range=0.5,shear_range=0.3,rescale=1/255,horizontal_flip=True)\n",
        "train = train_datagen.flow_from_directory(directory='drive/My Drive/paddy-disease-classification/train_images',target_size=(256,256),batch_size=32)\n"
      ],
      "metadata": {
        "id": "dWlOjhNfW82p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Showcasing three images from the dataset\n",
        "'''\n",
        "t_img,label = train.next()\n",
        "def plotImage(imgarr,label):\n",
        "  for im,l in zip(imgarr,label):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.imshow(im)\n",
        "    plt.show()\n",
        "\n",
        "plotImage(t_img[:3],label[:3])"
      ],
      "metadata": {
        "id": "3loQV8-dcYTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: About the CNN model:\n",
        "'''\n",
        "There are 12 layers total in this CNN model, 6 of which are conv2D levels and the remaining two are MaxPooling2D layers.\n",
        "They are alternately placed one on top of the other. The input shape is set at 256x256.\n",
        "I previously downsized the photos to 256x256, thus I'm using 256x256 for the input shape.\n",
        "\n",
        "Relu can be replaced with other terms like tanh, sigmoid, leaky relu, and many others.\n",
        "\n",
        "I'm using softmax at the output layer because our model is a multi-class classification model with a total of 10 classes.\n",
        "\n",
        "By the end of the 100th period, my accuracy was about 70%.\n",
        "\n",
        "I picked the Adam optimizer because it is essentially a standard optimizer that we employ.\n",
        "\n",
        "SGD, AdaGrad, AdaDelta, RMSProp, and other tools are alternatives to Adam.\n",
        "'''\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', input_shape=(256, 256, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128 (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(256, 256, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "])\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(train, epochs=100,steps_per_epoch=100)\n",
        "model.save('model_extended.h5')\n",
        "model.save('model.keras')"
      ],
      "metadata": {
        "id": "V1Bk8nMC7Tm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 4: About Model Prediction:\n",
        "'''\n",
        "I saved the model as model in the cell above.h5\n",
        "I'm importing the model into this cell and preparing it using a few different methods.\n",
        "\n",
        "the input image that will be used as a parameter in our model to determine its class.\n",
        "\n",
        "I'm reading the image file since I specified a 256x256 input dimension in the model design.\n",
        "\n",
        "transforming it into an array before scaling it down to 256x256.\n",
        "\n",
        "Prior to sending the photographs to the model, I additionally normalise them.\n",
        "\n",
        "The model result ranges from 0 to 9.\n",
        "\n",
        "To access the class of the image, I'm using it as the index value.\n",
        "\n",
        "'''\n",
        "img = load_img('/content/100042.jpg', target_size=(256, 256))\n",
        "img_array = img_to_array(img)\n",
        "img_array = img_array.reshape((1, 256, 256, 3))\n",
        "img_array = img_array / 255.0\n",
        "predictions = model.predict(img_array)\n",
        "predicted_class_index = predictions.argmax()\n",
        "class_labels = ['bacterial_leaf_blight', 'bacterial_leaf_streak', 'bacterial_panicle_blight','blast','brown_spot','dead_heart','downy_mildew','hispa','normal','tungro' ]  # Replace with your actual class labels\n",
        "predicted_class_label = class_labels[predicted_class_index]\n",
        "print(predicted_class_label)"
      ],
      "metadata": {
        "id": "K9aVaVspZ7rz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}